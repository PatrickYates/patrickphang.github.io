<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[Git常见操作(完整提交步骤)]]></title>
      <url>%2F2017%2F01%2F03%2FGit%E5%B8%B8%E8%A7%81%E6%93%8D%E4%BD%9C-%E5%AE%8C%E6%95%B4%E6%8F%90%E4%BA%A4%E6%AD%A5%E9%AA%A4%2F</url>
      <content type="text"><![CDATA[从已有的Git仓库中克隆一个本地的镜像仓库：`git clone https://github.com/YourAccountName/name.git` 自己新建一个仓库并上传到远程仓库： git init -&gt; 初始化新的仓库 git add README.md -&gt; 将新加入的untracked状态的文件加入跟踪并放入暂存区///将已修改的文件放入暂存区git add . -&gt; 操作所有文件 12345678On branch masterInitial commitChanges to be committed: (use &quot;git rm --cached &lt;file&gt;...&quot; to unstage) new file: a01.txt git commit -m &quot;第一次提交&quot; -&gt; 将暂存区中的文件提交至HEAD所指向的分支，暂存区的文件将回到未修改状态 123[master (root-commit) af05048] 第一次 1 file changed, 3 insertions(+) create mode 100644 a01.txt 当修改文件时: 123456789101112131415161718192021222324252627282930313233343536 On branch master Changes not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) modified: a01.txt no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;) Untracked files: (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) a02.txt no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)``` 其他常见方法： `git status` -&gt; 查看仓库中文件的状态 `git rm a01.txt` -&gt; 将已跟踪的文件从Git中移除 `git checkout -- a01.txt` -&gt; 恢复被删除的文件(未被commit) `git reset HEAD readme.txt` -&gt; 恢复被修改的文件(未被commit),并不会更改文件内容，只是使之回到已修改状态 `git commit --amend -m a01.txt` -&gt; 恢复被删除的文件(已被commit)4. `git remote add origin https://github.com/YourAccountName/name.git` -&gt; 这样就可以用origin这个名字来引用添加的远程仓库 5. `git fetch origin` -&gt; 并不能看到工作目录下有任何变化，只是把远程的数据抓取到本地，而不会把改动合并到当前的分支上 `git pull https://github.com/YourAccountName/name.git` -&gt; 把远程仓库抓取到本地，并合并本地master分支6. `git push -u origin master` -&gt; 将本地的数据更新到远程仓库中 `git push` -&gt; 以后直接使用----------------##遇到的错误(慢慢更新......)### 1. git push origin master To https://github.com/PatrickYates/patrickyates.github.com.git ! [rejected] master -&gt; master (non-fast-forward)error: failed to push some refs to ‘https://github.com/PatrickYates/patrickyates.github.com.git’hint: Updates were rejected because the tip of your current branch is behindhint: its remote counterpart. Integrate the remote changes (e.g.hint: ‘git pull …’) before pushing again.hint: See the ‘Note about fast-forwards’ in ‘git push –help’ for details.12345* 解决办法：当前的本地仓库不是最新的，应该pull到本地### 2. git pull https://github.com/YourAccountName/name.git From https://github.com/YourAccountName/name.git branch HEAD -&gt; FETCH_HEADfatal: refusing to merge unrelated histories12 git pull https://github.com/YourAccountName/name.gitwarning: no common commitsremote: Counting objects: 60, done.remote: Compressing objects: 100% (51/51), done.remote: Total 60 (delta 1), reused 60 (delta 1), pack-reused 0Unpacking objects: 100% (60/60), done.From https://github.com/YourAccountName/name.git branch HEAD -&gt; FETCH_HEADfatal: refusing to merge unrelated histories``` 解决办法：本地代码与远程仓库代码完全不同，无法合并 其他Github精华教程： 交互编程-15分钟学会github 书籍-重量级教程progit 书籍-git magic 教程-如何高效利用GitHub 教程-git immersion中文版 参考资料 必须要会的Git基本使用及常用命令操作]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[reqwest库的使用]]></title>
      <url>%2F2017%2F01%2F03%2Freqwest%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
      <content type="text"><![CDATA[1234567891011121314151617181920212223&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;meta charset="utf-8"&gt; &lt;title&gt;&lt;/title&gt; &lt;script src="https://cdnjs.cloudflare.com/ajax/libs/reqwest/2.0.5/reqwest.js"&gt;&lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;Hello There!&lt;/h1&gt; &lt;script&gt; var a = &#123;&#125; reqwest(&#123; url:"http://swapi.co/api/people/1/", type:"json", method:"get", data:&#123;tag:"life"&#125;, success:function (resp) &#123; a = resp &#125; &#125;) &lt;/script&gt; &lt;/body&gt;&lt;/html&gt;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Python爬虫爬取极客学院]]></title>
      <url>%2F2017%2F01%2F03%2FPython%E7%88%AC%E8%99%AB%E7%88%AC%E5%8F%96%E6%9E%81%E5%AE%A2%E5%AD%A6%E9%99%A2%2F</url>
      <content type="text"><![CDATA[原文：github 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131import urllib.request, urllib.parse, os, urllib, http.cookiejar, re# 下载极客学院的视频# 需要一个vip账号(验证邮箱和手机会有体验vip)class DownCourse(object): # 给urllib添加cookie支持 # path: 下载的视频要保存的文件夹 def __init__(self,path): # 初始化一个CookieJar来处理Cookie cookieJar = http.cookiejar.CookieJar() # 实例化一个全局opener opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cookieJar)) # 把这个cookie处理机制装上去,大概是这个意思-.- urllib.request.install_opener(opener) self.folderPath = path # 判断文件夹是否存在 folderExists = os.path.exists(self.folderPath) if not folderExists: os.mkdir(self.folderPath) # 登陆函数 def login(self): # 从登录页面获取登陆参数 login_url = 'http://passport.jikexueyuan.com/sso/login' # 登陆信息发送到这个地址 passport_url = 'http://passport.jikexueyuan.com/submit/login?is_ajax=1' verifyCode_url = 'http://passport.jikexueyuan.com/sso/verify' # 获取登陆页面源码 request = urllib.request.urlopen(login_url) html = request.read() request.close() # 获取登陆要post的数据 expire = re.search(r"(?s)value='(.*?)' name='expire",html) # 验证码 verifyCodeGifPath = '/tmp/jikexueyuan.gif' request = urllib.request.urlopen(verifyCode_url) gif = request.read() request.close() fGif = open(verifyCodeGifPath,'w') fGif.write(gif) fGif.close() # 读取保存到本地的验证码图片 os.system('eog ' + verifyCodeGifPath) verify = input("请输入图中的验证码:") data = &#123; 'expire': expire.group(1), 'referer': 'http%3A%2F%2Fwww.jikexueyuan.com%2F', 'uname': 15850673601, 'password': pengxiaoye, 'verify': verify, &#125; post_data = urllib.parse.urlencode(data) request = urllib.request.Request(passport_url,post_data) # 给一个useragent,防止被认为是爬虫程序 request.add_header('User-Agent', 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.125 Safari/537.36') # 发送登录请求 request = urllib.request.urlopen(request) request.close() print('登陆完成') # courseUrl: 课程地址首页,例如:http://www.jikexueyuan.com/course/989.html def download(self, courseUrl): # 获取课程名称 request = urllib.request.urlopen(courseUrl) coursePageHtml = request.read() request.close() courseName = re.search(r'(?s)&lt;title&gt;(.*?)-',coursePageHtml).group(1) # 课程数量 courseCount = int(re.search(r'(?s)class="timebox"&gt;&lt;span&gt;(.*?)课时',coursePageHtml).group(1)) # 存储视频的文件夹路径 folderPath = self.folderPath + courseName + '/' # 判断文件夹是否存在 folderExists = os.path.exists(folderPath) if not folderExists: os.mkdir(folderPath) print('课程名:' + courseName + ' 课程数量:' + str(courseCount)) # 课程的编号,构建课程的页面地址 i = 0 while i &lt; courseCount: i += 1 pageUrl = courseUrl.split('.html')[0] + '_' + str(i) + '.html?ss=1' # 本节课程的html代码 request = urllib.request.urlopen(pageUrl) pageHtml = request.read() request.close() # 本节课程的名称 name = re.search(r'(?s)&lt;title&gt;(.*?)-',pageHtml).group(1) # 本节课程的视频地址 videoUrl = re.search(r'&lt;source src="(.*?)"',pageHtml) # 有的页面写的课时比实际课时多,会匹配不到视频地址 if videoUrl == None: continue else: videoUrl = videoUrl.group(1) print('正在下载' + name + '...') # 存储视频的Path: 总路径/课程名/每一节的名称 urllib.urlretrieve(videoUrl,folderPath + str(i) + name + '.mp4',self.cbk) print('下载完成') # 从网上下载的可以显示下载进度的函数 # \b是我加的,产生了很奇特的显示效果,还行 def cbk(self,a, b, c): per = 100.0*a*b/c if per &gt; 100: per = 100 print('%.2f%%\b\b\b\b\b\b' % per) def cbk(self,a,b,c): per = 100.0 * a * b /c if per &gt;100: per = 100 print('%.2f%%\b\b\b\b\b\b' % per)# 建立下载对象,参数是即将下载的这些视频放的目录,程序会根据课程名在这个文件夹里面再建文件夹down = DownCourse('/home/geekgao/视频/SpringMVC/')down.login()# 下载一个页面中的所有课程request = urllib.request.urlopen('http://www.jikexueyuan.com/course/springmvc/')html = request.read()request.close()courseUrls = re.findall(r'class="lesson-info-h2"&gt;&lt;a href="(.*?)"',html)for courseUrl in courseUrls: down.download(courseUrl)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Python爬虫实战：极客学院]]></title>
      <url>%2F2017%2F01%2F03%2FPython%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%98%EF%BC%9A%E6%9E%81%E5%AE%A2%E5%AD%A6%E9%99%A2%2F</url>
      <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#_*_coding:utf-8_*_ from lxml import etree import requests import sys reload(sys) sys.setdefaultencoding("utf-8") #把课程信息保存到info.txt中 def saveinfo(classinfo): f = open('info.txt','a') f.writelines('title:'+ classinfo['title']+'\n') f.writelines('content:' + classinfo['content'] + '\n') f.writelines('classtime:' + classinfo['classtime'] + '\n') f.writelines('classlevel:' + classinfo['classlevel'] + '\n') f.writelines('learnnum:' +classinfo['learnnum'] +'\n\n') f.close() #爬虫主体 def spider(url): html = requests.get(url) #用Requests下载网页 selector = etree.HTML(html.text) #以下用Xpath来解析网页 content_field = selector.xpath('//div[@class="lesson-list"]/ul/li') info = [] for each in content_field: classinfo = &#123;&#125; classinfo['title'] = each.xpath('div[@class="lesson-infor"]/h2[@class="lesson-info-h2"]/a/text()')[0] classinfo['content'] = (each.xpath('div[@class="lesson-infor"]/p/text()')[0]).strip() classTime = (each.xpath('div[@class="lesson-infor"]/div/div/dl/dd[@class="mar-b8"]/em/text()')[0]).split() classinfo['classtime'] = ''.join(classTime) classinfo['classlevel'] = each.xpath('div[@class="lesson-infor"]/div/div/dl/dd[@class="zhongji"]/em/text()')[0] classinfo['learnnum'] = each.xpath('div[@class="lesson-infor"]/div[@class="timeandicon"]/div/em/text()')[0] info.append(classinfo) return info if __name__ == '__main__': print u'开始爬取内容。。。' page = [] #循环用来生产不同页数的链接 for i in range(1,11): newpage = 'http://www.jikexueyuan.com/course/?pageNum=' + str(i) print u"第%d页"%i print u'正在处理页面：'+ newpage page.append(newpage) for each in page: info = spider(each) for each in info: saveinfo(each) #转自Python爬虫实战：极客学院]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python利用百度天气api查询天气数据]]></title>
      <url>%2F2017%2F01%2F03%2Fpython%E5%88%A9%E7%94%A8%E7%99%BE%E5%BA%A6%E5%A4%A9%E6%B0%94api%E6%9F%A5%E8%AF%A2%E5%A4%A9%E6%B0%94%E6%95%B0%E6%8D%AE%2F</url>
      <content type="text"><![CDATA[一.说明本次实验的是百度天气的api,网址为：click here查询广州的天气，返回json格式，然后解析内容（上面链接点开就知道了）。(可以使用chrome应用Postman来pretiffy json内容) 二.示例代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263import requestsimport urllib.requestimport http.clientimport json#1.requests内置json解析方法url = 'http://api.map.baidu.com/telematics/v3/weather?location=%E5%B9%BF%E5%B7%9E&amp;output=json&amp;ak=KPGX6sBfBZvz8NlDN5mXDNBF&amp;callback='r = requests.get(url)print("查询日期："+r.json['date'])print("城市："+r.json['results'][0]['currentCity'])print("pm2.5值："+r.json['results'][0]['pm25'])print("\n")for i in range(0,4): print("日期："+r.json['results'][0]['weather_data'][i]['date']) print("天气："+r.json['results'][0]['weather_data'][i]['weather']) print("风力："+r.json['results'][0]['weather_data'][i]['wind']) print("温度："+r.json['results'][0]['weather_data'][i]['temperature']) print("---------------")#2.urllib库 + json库的json.loads()url = 'http://api.map.baidu.com/telematics/v3/weather?location=%E5%B9%BF%E5%B7%9E&amp;output=json&amp;ak=KPGX6sBfBZvz8NlDN5mXDNBF&amp;callback='request = urllib.request.urlopen(url).read().decode('utf8')) #注意必须要.decode('utf8')，不然会有错误：the JSON object must be str, not 'bytes's = json.loads(request)print("查询日期："+s['date'])print("城市："+s['results'][0]['currentCity'])print("pm2.5值："+s['results'][0]['pm25'])print("\n")for i in range(0,4): print("日期："+s['results'][0]['weather_data'][i]['date']) print("天气："+s['results'][0]['weather_data'][i]['weather']) print("风力："+s['results'][0]['weather_data'][i]['wind']) print("温度："+s['results'][0]['weather_data'][i]['temperature']) print("---------------")#3.http.Client库 + json库的json.loads()url = 'http://api.map.baidu.com/telematics/v3/weather?location=%E5%B9%BF%E5%B7%9E&amp;output=json&amp;ak=KPGX6sBfBZvz8NlDN5mXDNBF&amp;callback='httpClient = http.client.HTTPConnection('api.map.baidu.com', 80, timeout=30)httpClient.request('GET', '/telematics/v3/weather?location=%E5%B9%BF%E5%B7%9E&amp;output=json&amp;ak=KPGX6sBfBZvz8NlDN5mXDNBF&amp;callback=')response = httpClient.getresponse()s = json.loads(response.read().decode('utf8')) #注意必须要.decode('utf8')，不然会有错误：the JSON object must be str, not 'bytes'print("查询日期："+s['date'])print("城市："+s['results'][0]['currentCity'])print("pm2.5值："+s['results'][0]['pm25'])print("\n")for i in range(0,4): print("日期："+s['results'][0]['weather_data'][i]['date']) print("天气："+s['results'][0]['weather_data'][i]['weather']) print("风力："+s['results'][0]['weather_data'][i]['wind']) print("温度："+s['results'][0]['weather_data'][i]['temperature']) print("---------------") 三.自己定义json解析函数待解析json片段：1234567891011121314151617181920212223242526272829303132&#123; "showapi_res_code": 0, "showapi_res_error": "", "showapi_res_body": &#123; "pagebean": &#123; "allNum": 5034, "allPages": 252, "contentlist": [ &#123; "code2img": "http://app1.showapi.com/weixin_info/pubNum/xxxxxx.jpg", "id": "55cbfce16e36a9c5946e40b0", "pubNum": "xxxx", "tag": "", "type1_id": "44", "type1_name": "名人明星", "type2_id": "73", "type2_name": "时尚", "userLogo": "http://app1.showapi.com/weixin_info/pubNum/xxxx.jpg", "weiNum": "xxx66 " &#125;, &#123; "code2img": "http://app1.showapi.com/weixin_info/pubNum/xxxx.jpg", "id": "55cbfcdf6e36a9c5946e40ae", "pubNum": "阳西县蓝星半岛旅游度假村", "tag": "添加微信号:xxxx22 ", "type1_id": "47", "type1_name": "生活购物", "type2_id": "100", "type2_name": "旅游", "userLogo": "http://app1.showapi.com/weixin_info/pubNum/xxxx.jpg", "weiNum": "xxxx22" &#125; 函数：12345678910111213141516def json_path(d, path, sep='.'): pp = path.split(sep) t = d for p in pp: if type(t) is dict: t = t[p] elif type(t) is list: t = t[int(p)] else: t = None return timport jsond = json.loads(s)print json_path(d, "showapi_res_body.pagebean.contentlist.1.pubNum") #阳西县蓝星半岛旅游度假村print json_path(d, "showapi_res_code") # 0 四.参考资料 知乎-为什么我已经知道了python的基本语法，可还是不会写个类似天气预报或者能聊天的小软件？ python结合API实现即时天气信息 python 调用 API 获得的JSON如何处理才能获得我想获得的内容呢? 网上的天气 API 哪一个更加可靠？ python day 08获取天气信息.制作天气预报软件]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[pyquery库的使用]]></title>
      <url>%2F2017%2F01%2F03%2Fpyquery%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
      <content type="text"><![CDATA[#coding=utf-8 #防止报错：UnicodeEncodeError: ‘gbk’ codec can’t encode characterfrom pyquery import PyQuery as pqfrom lxml import etree 可加载一段HTML字符串，或一个HTML文件，或是一个url地址，或lxml.etree：12345678htmlblock = "&lt;html&gt;&lt;title&gt;hello&lt;/title&gt;&lt;/html&gt;"filename = "path_to_html_file"url = "http://www.baidu.com"d = pq(htmlblock)d = pq(filename)d = pq(url)d = pq(etree.fromstring("&lt;html&gt;&lt;/html&gt;")) 直接输出截取串的html对象，看着更加直观 v_source = pq(url=&apos;http://yunvs.com/list/mai_1.html&apos;) for data in v_source(&apos;tr&apos;): print(pq(data).html()) print(pq(data).text()) #以text文本的方式输出，这样就去掉了html标记 pq(d)(‘a[class= “”]’).attr(‘’)Aurl= pq(data)(&#39;a[class= &quot;j_th_tit &quot; ]&#39;).attr(&#39;href&#39;) 常用方法：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768#1.html()和text() ——获取相应的HTML块或文本块d = pq("&lt;head&gt;&lt;title&gt;hello&lt;/title&gt;&lt;/head&gt;")d('head').html() #返回&lt;title&gt;hello&lt;/title&gt;d('head').text() #返回hello#2.根据HTML标签获取元素。注意：当获取到的元素不只一个时，html()、text()方法只返回首个元素的相应内容块d = pq('&lt;div&gt;&lt;p&gt;test 1&lt;/p&gt;&lt;p&gt;test 2&lt;/p&gt;&lt;/div&gt;')print(d('p')) #返回&lt;p&gt;test 1&lt;/p&gt;&lt;p&gt;test 2&lt;/p&gt;print(d('p').html()) #返回test 1#3.eq(index) ——根据给定的索引号得到指定元素。接上例，若想得到第二个p标签内的内容，则可以：print(d('p').eq(1).html()) #返回test 2#4.filter() ——根据类名、id名得到指定元素，例：d = pq("&lt;div&gt;&lt;p id='1'&gt;test 1&lt;/p&gt;&lt;p class='2'&gt;test 2&lt;/p&gt;&lt;/div&gt;")d('p').filter('#1') #返回[&lt;p#1&gt;]d('p').filter('.2') #返回[&lt;p.2&gt;]#5.find() ——查找嵌套元素，例：d = pq("&lt;div&gt;&lt;p id='1'&gt;test 1&lt;/p&gt;&lt;p class='2'&gt;test 2&lt;/p&gt;&lt;/div&gt;")d('div').find('p')#返回[&lt;p#1&gt;, &lt;p.2&gt;]d('div').find('p').eq(0)#返回[&lt;p#1&gt;]#6.直接根据类名、id名获取元素，例：d = pq("&lt;div&gt;&lt;p id='1'&gt;test 1&lt;/p&gt;&lt;p class='2'&gt;test 2&lt;/p&gt;&lt;/div&gt;")d('#1').html()#返回test 1d('.2').html()#返回test 2#7.获取属性值，例：d = pq("&lt;p id='my_id'&gt;&lt;a href='http://hello.com'&gt;hello&lt;/a&gt;&lt;/p&gt;")d('a').attr('href')#返回http://hello.comd('p').attr('id')#返回my_id#8.修改属性值，例：d('a').attr('href', 'http://baidu.com')把href属性修改为了baidu#9.addClass(value) ——为元素添加类，例：d = pq('&lt;div&gt;&lt;/div&gt;')d.addClass('my_class')#返回[&lt;div.my_class&gt;]#10.hasClass(name) #返回判断元素是否包含给定的类，例：d = pq("&lt;div class='my_class'&gt;&lt;/div&gt;")d.hasClass('my_class')#返回True#11.children(selector=None) ——获取子元素，例：d = pq("&lt;span&gt;&lt;p id='1'&gt;hello&lt;/p&gt;&lt;p id='2'&gt;world&lt;/p&gt;&lt;/span&gt;")d.children()#返回[&lt;p#1&gt;, &lt;p#2&gt;]d.children('#2')#返回[&lt;p#2&gt;]#12.parents(selector=None)——获取父元素，例：d = pq("&lt;span&gt;&lt;p id='1'&gt;hello&lt;/p&gt;&lt;p id='2'&gt;world&lt;/p&gt;&lt;/span&gt;")d('p').parents() #返回[&lt;span&gt;]d('#1').parents('span') #返回[&lt;span&gt;]d('#1').parents('p') #返回[]#13.clone() ——返回一个节点的拷贝#14.empty() ——移除节点内容#15.nextAll(selector=None) ——返回后面全部的元素块，例：d = pq("&lt;p id='1'&gt;hello&lt;/p&gt;&lt;p id='2'&gt;world&lt;/p&gt;&lt;img scr='' /&gt;")d('p:first').nextAll()#返回[&lt;p#2&gt;, &lt;img&gt;]d('p:last').nextAll()#返回[&lt;img&gt;]#16.not_(selector) ——返回不匹配选择器的元素，例：d = pq("&lt;p id='1'&gt;test 1&lt;/p&gt;&lt;p id='2'&gt;test 2&lt;/p&gt;")d('p').not_('#2')#返回[&lt;p#1&gt;] 与requests库结合使用：123456import requestsfrom pyquery import PyQuery as pqr = requests.get('http://www.meipai.com/media/596371059')d = pq(r.content)print(d('meta[property="og:video:url"]').attr('content')) 参考资料PyQuery 1.2.4 complete APIpyquery: 基于python和jquery语法操作XML这一年Python总结Python爬虫利器六之PyQuery的用法]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[BeautifulSoup库的使用]]></title>
      <url>%2F2017%2F01%2F03%2FBeautifulSoup%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
      <content type="text"><![CDATA[导入所需包from bs4 import BeautifulSoupsoup = BeautifulSoup(html) 解析顺序‘lxml’-&gt;’html5lib’-&gt;’html.parser’ 对象种类有四种类型：Tag，NavigableString，BeautifulSoup，Comment。BeautifulSoup将文档转化为树形结构，每个节点都是上述四种类型的Python对象。 遍历文档树BeautifulSoup对象作为一棵树，有多个节点。对于一个节点，相对于它所在的位置，有子节点、父节点、兄弟节点。 1. 子节点* 一个Tag可包含多个Tag以及字符串，这些都是这个Tag的子节点。而NavigableString不会有子节点。 * 如果想要获得某个Tag：`soup.tag_name` * 通过点取属性，只能获得当前名字的第一个tag，若要获取所有，需要使用搜索文档树中的方法:`soup.find_all(&apos;tag_name&apos;)` * tag的.contents属性可将所有子节点以列表的方式输出。可通过tag的.children生成器，对所有子节点遍历。 * `.contents`和`.children`只对获取Tag的直接子节点，`.descendants`可对Tag的所有子孙节点遍历。 * 如果tag只有一个NavigableString类型子节点，则可用`.string`获取。如果包含多个，使用`.strings`遍历。若输出的字符串中包含空格或空行，使用`.stripped_strings`去除。 12res = soup.stripped_stringsprint(list(res)) 2. 父节点.parent -&gt; 当前节点的父节点.parents -&gt; 当前节点的所有父辈节点 3. 兄弟节点拥有同一父节点的节点之间：.next_sibling.previous_sibling 所有兄弟节点： `.next_siblings` `.previous_siblings` 指向下一个或上一个解析对象： `.next_element` `.previous_element` `.next_elements` `.previous_elements` 搜索文档树：find(str)和find_all(str) 其中的str，代表了tag的name。可以是纯字符串、正则表达式、列表（任一匹配就满足条件，是或运算）、True（返回所有Tag节点不返回字符串节点）。另一种入参不是str，而是method。此方法是一个函数，只接受一个元素入参，若此函数返回True表示入参匹配要求。例如：def has_class_but_no_id(tag):return tag.has_attr(‘class’) and not tag.has_attr(‘id’)综上，过滤器包括：纯字符串、正则表达式、列表、True、方法这几种。 1. find_all(name,attrs,recursive,text,**kwargs)该方法搜索当前节点的所有tag子节点。 name参数：指的是tag的name属性，字符串对象自动忽略。过滤器可以使用全部种类。 keyword参数：如果一个入参指定了名字，但是并不是上述提到的入参名字，搜索时会把该入参当做是tag的属性来搜索。例如：soup.find_all(id=&#39;link2&#39;)会返回tag中存在属性id，并且id对应的值是link2的tag。以上方法可使用除方法之外的所有过滤器。某些特殊属性不能这样直接使用，则使用如下方法：soup.find_all(attrs={&quot;key&quot;:&quot;value&quot;})例如要使用class属性进行搜索，由于class是python中的保留字，不能直接写成入参，目前有两种方法：soup.findall(‘tag.name’,class=’class_value’)soup.find_all(‘tag.name’,attrs={‘class’:’classvalue’})class方法可以使用全部过滤器。另外，因为class是一个多值属性，所以只需要匹配一个值，就可以得到结果，所谓的不完全匹配。使用完全匹配时，过滤器中的字符顺序需要和实际相符合才能得到对应结果。 text参数：搜索的是Tag中的字符串内容，可使用全部过滤器。 limit参数：限制返回数量。 recursive参数：find_all()默认是搜索当前节点的所有子孙节点，若只需要搜索直接的子节点，则设置recursive=False。 find_all()是实际当中用的最广泛的。因此有了等价的简化版：soup.find_all(&#39;a&#39;)或soup(&#39;a&#39;) 2. find(name,attrs,recursive,text,**kwargs)find()方法等价于find_all(limit=1)，返回符合条件的第一个对象。区别在于，前者直接返回结果，后者返回只有一个元素的列表。若没有对象符合条件，前者返回None，后者返回空列表。简化版：soup.find(&#39;head&#39;).find(&#39;title&#39;)或soup.head.title 除了find()和find_all()之外还有一些搜索的方法： find_parent() find_next_sibling() find_previous_sibling() 上面三种可以在后面加&apos;s&apos;表示所有。 find_next() find_previous() find_all_next() find_all_previous() 3. CSS选择器(.select()方法)Tag或BeautifulSoup对象的.select()方法。res = soup.select(&#39;#wrapperto&#39;) -&gt; tag’s idres = soup.select(&#39;img[src]&#39;) -&gt; ‘img’ tags有’src’ attributesres = soup.select(&#39;img[src=...]&#39;) -&gt; ‘src’ attributes是… 输出 soup.prettify()将文档树格式化之后输出。 若不注重格式，则可使用python的str()或unicode()。 如果想得到tag中包含的文本内容，使用get_text()，可获取到当前节点的文本，以及子孙节点中的文本。返回的是Unicode。 可以指定参数设置分隔符如get_text(&quot;|&quot;)是以“|”作为分隔符。 get_text(strip=True)可去除文本前后的空白。 或者用.stripped_strings进行遍历。获得父级标签下的所有子标签内的文本信息，相当于处理多个文本的高级的get_text()方法 编码1.soup使用Unicode编码。2.BeautifulSoup对象的.original_encoding属性来获取自动识别编码的结果。3.在创建BeautifulSoup对象时，指定入参from_encoding来告知文档的编码方式。4.有时转码时有些特殊字符替换成了特殊的Unicode，可通过BeautifulSoup对象的.contains_repalcement_characters属性来判断是否有此情况，为True即为有特殊替换。5.输出编码统一为UTF8，若想要其他的编码，则和一般的python字符串相同，需要进行手动设置。 完整实例代码123456789101112131415161718192021222324252627import requests,urllib.requestfrom bs4 import BeautifulSoupsource_code = requests.get(url,headers=header)wb_data = source_code.textsoup = BeautifulSoup(wb_data,'lxml')titles = soup.select('body &gt; div.main-content &gt; ul &gt; li &gt; div.article-info &gt; h3 &gt; a')images = soup.select('body &gt; div.main-content &gt; ul &gt; li &gt; img')#获取网页信息for title,image in zip(titles,images): data = &#123; 'title':title.get_text(), 'image':image.get('src') &#125;print(data)#下载download_links = []folder_path = "C:/Users/asus-pc/Desktop"for pic_tag in soup.find_all('img'): pic_link = pic_tag.get('src') download_links.append(pic_link)for item in download_links: urllib.request.urlretrieve(item,folder_path+item[-5:]) print("done!")]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[requests库的使用]]></title>
      <url>%2F2017%2F01%2F03%2Frequests%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
      <content type="text"><![CDATA[import requests -&gt; 引用模块 1. 发送请求与传递参数带参数请求：12345678910111213141516171819202122#GET参数实例requests.get('http://www.dict.baidu.com/s', params=&#123;'wd': 'python'&#125;) #或url = 'http://www.baidu.com'payload = &#123;'key1': 'value1', 'key2': 'value2'&#125;headers = &#123; "Accept":"text/html,application/xhtml+xml,application/xml;", "Accept-Encoding":"gzip", "Accept-Language":"zh-CN,zh;q=0.8", "Referer":"http://www.example.com/", "User-Agent":"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.90 Safari/537.36" &#125; res1 = requests.get(url, params=payload, headers=headers, timeout=1)#POST参数实例requests.post('http://www.itwhy.org/wp-comments-post.php', data=&#123;'comment': '测试post'&#125;)files = &#123;'file': open('touxiang.png', 'rb')&#125; #用于发送文件的post属性files = &#123;'file': ('xxxx,jpg',open('/home/lyb/sjzl.mpg','rb'))&#125; #设置文件名#或url = 'http://www.baidu.com'data=&#123;"user":"user","password":"pass"&#125;res2 = requests.post(url1, data=data, headers=headers ,files=files) POST发送JSON数据：12345import jsonr = requests.post('https://api.github.com/some/endpoint', data=json.dumps(&#123;'some': 'data'&#125;))print(r.json()) 12r = requests.get('http://ip.taobao.com/service/getIpInfo.php?ip=122.88.60.28')print (r.json()['data']['country']) 添加代理：12345proxies = &#123; "http": "http://10.10.1.10:3128", "https": "http://10.10.1.10:1080",&#125;requests.get("http://www.zhidaow.com", proxies=proxies) 一些操作requests返回值的方法：12345678910111213141516r.text #字符串方式的响应体，会自动根据响应头部的字符编码进行解码r.content #获得二进制响应内容r.raw #获得原始响应内容,需要stream=Truer.raw.read(50)type(r.text) #返回解码成unicode的内容r.urlr.history #追踪重定向r.cookiesr.cookies['example_cookie_name']r.headers #以字典对象存储服务器的响应头，但该字典比较特殊，字典键不区分大小写，若键不存在返回Noner.headers['Content-Type']r.headers.get('content-type')r.json #讲返回内容编码为jsonr.encoding #返回内容编码r.status_code #返回http状态码r.raise_for_status() #返回错误状态码 若编码出错，则r.text.encode(&#39;utf-8&#39;) Session()12345678910#初始化一个session对象s = requests.Session()#使用这个session对象来进行访问prepped1 = requests.Request('POST', url1, data=data, headers=headers).prepare()s.send(prepped1)#或 r = s.post(url,data = user) 其他的一些访问方式： &gt;&gt;&gt; r = requests.put(&quot;http://httpbin.org/put&quot;) &gt;&gt;&gt; r = requests.delete(&quot;http://httpbin.org/delete&quot;) &gt;&gt;&gt; r = requests.head(&quot;http://httpbin.org/get&quot;) &gt;&gt;&gt; r = requests.options(&quot;http://httpbin.org/get&quot;) 总结该笔记描述那么多方面，好像只是讲了requests模块的参数而已。这也说明了它的强大，但是前提是你必须懂对应的原理。参数如下： json: json数据传到requests的body headers: HTTP Headers的字典传到requests的header cookies: 可以使用字典或者CookieJar object files: 字典{&apos;name&apos;: file-tuple} 来实现multipart encoding upload, 2参数元组 (&apos;filename&apos;, fileobj), 3参数元组 (&apos;filename&apos;, fileobj, &apos;content_type&apos;)或者 4参数元组 (&apos;filename&apos;, fileobj, &apos;content_type&apos;, custom_headers), 其中&apos;content-type&apos; 用于定于文件类型和custom_headers文件的headers auth: Auth元组定义用于Basic/Digest/Custom HTTP Auth timeout: 连接等待时长 allow_redirects: 布尔型， True代表POST/PUT/DELETE只有的重定向是允许的 proxies: 代理的地址 verify: 用于认证SSL证书 stream: False代表返回内容立刻下载 cert: String代表ssl client证书地址(.pem) Tuple代表(&apos;cert&apos;, &apos;key&apos;)键值对 其他参考资料：网页数据压缩deflate&amp;gzip]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[lxml学习笔记]]></title>
      <url>%2F2017%2F01%2F03%2Flxml%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
      <content type="text"><![CDATA[问题1：有一个XML文件，如何解析问题2：解析后，如果查找、定位某个标签问题3：定位后如何操作标签，比如访问属性、文本内容等from lxml import etree -&gt; 导入模块，该库常用的XML处理功能都在lxml.etree中 requests + lxml解析小12345678910from lxml import etree import requests page = 1 url = 'http://www.jikexueyuan.com/course/?pageNum=' + str(page) html = requests.get(url) selector = etree.HTML(html.text) content_field = selector.xpath('//div[@class="lesson-list"]/ul/li') print content_field Element类 Element是XML处理的核心类，Element对象可以直观的理解为XML的节点，大部分XML节点的处理都是围绕该类进行的。这部分包括三个内容：节点的操作、节点属性的操作、节点内文本的操作。 1. 节点操作 创建Element对象使用Element方法，参数即节点名称。 123&gt;&gt;&gt; root = etree.Element('root')&gt;&gt;&gt; print(root)&lt;Element root at 0x2da0708&gt; 获取节点名称使用tag属性，获取节点的名称。 12&gt;&gt;&gt; print(root.tag)root 输出XML内容使用tostring方法输出XML内容，参数为Element对象。 12&gt;&gt;&gt; print(etree.tostring(root))b'&lt;root&gt;&lt;child1/&gt;&lt;child2/&gt;&lt;child3/&gt;&lt;/root&gt;' 添加子节点使用SubElement方法创建子节点，第一个参数为父节点（Element对象），第二个参数为子节点名称。 123&gt;&gt;&gt; child1 = etree.SubElement(root, 'child1')&gt;&gt;&gt; child2 = etree.SubElement(root, 'child2')&gt;&gt;&gt; child3 = etree.SubElement(root, 'child3') 删除子节点使用remove方法删除指定节点，参数为Element对象。clear方法清空所有节点。 123456&gt;&gt;&gt; root.remove(child1) # 删除指定子节点&gt;&gt;&gt; print(etree.tostring(root))b'&lt;root&gt;&lt;child2/&gt;&lt;child3/&gt;&lt;/root&gt;'&gt;&gt;&gt; root.clear() # 清除所有子节点&gt;&gt;&gt; print(etree.tostring(root))b'&lt;root/&gt;' 以列表的方式操作子节点可以将Element对象的子节点视为列表进行各种操作： 12345678910111213141516171819202122232425262728&gt;&gt;&gt; child = root[0] # 下标访问&gt;&gt;&gt; print(child.tag)child1&gt;&gt;&gt; print(len(root)) # 子节点数量3&gt;&gt;&gt; root.index(child2) # 获取索引号1&gt;&gt;&gt; for child in root: # 遍历... print(child.tag)child1child2child3&gt;&gt;&gt; root.insert(0, etree.Element('child0')) # 插入&gt;&gt;&gt; start = root[:1] # 切片&gt;&gt;&gt; end = root[-1:]&gt;&gt;&gt; print(start[0].tag)child0&gt;&gt;&gt; print(end[0].tag)child3&gt;&gt;&gt; root.append( etree.Element('child4') ) # 尾部添加&gt;&gt;&gt; print(etree.tostring(root))b'&lt;root&gt;&lt;child0/&gt;&lt;child1/&gt;&lt;child2/&gt;&lt;child3/&gt;&lt;child4/&gt;&lt;/root&gt;' 获取父节点使用getparent方法可以获取父节点。 12&gt;&gt;&gt; print(child1.getparent().tag)root 属性操作属性是以key-value的方式存储的，就像字典一样。 1. 创建属性可以在创建Element对象时同步创建属性，第二个参数即为属性名和属性值：12345678&gt;&gt;&gt; root = etree.Element('root', interesting='totally')&gt;&gt;&gt; print(etree.tostring(root))b'&lt;root interesting="totally"/&gt;'也可以使用set方法给已有的Element对象添加属性，两个参数分别为属性名和属性值：&gt;&gt;&gt; root.set('hello', 'Huhu')&gt;&gt;&gt; print(etree.tostring(root))b'&lt;root interesting="totally" hello="Huhu"/&gt;' 2. 获取属性属性是以key-value的方式存储的，就像字典一样。直接看例子12345678910111213# get方法获得某一个属性值&gt;&gt;&gt; print(root.get('interesting'))totally# keys方法获取所有的属性名&gt;&gt;&gt; sorted(root.keys())['hello', 'interesting']# items方法获取所有的键值对&gt;&gt;&gt; for name, value in sorted(root.items()):... print('%s = %r' % (name, value))hello = 'Huhu'interesting = 'totally' 也可以用attrib属性一次拿到所有的属性及属性值存于字典中：1234567&gt;&gt;&gt; attributes = root.attrib&gt;&gt;&gt; print(attributes)&#123;'interesting': 'totally', 'hello': 'Huhu'&#125;&gt;&gt;&gt; attributes['good'] = 'Bye' # 字典的修改影响节点&gt;&gt;&gt; print(root.get('good'))Bye 文本操作标签及标签的属性操作介绍完了，最后就剩下标签内的文本了。可以使用text和tail属性、或XPath的方式来访问文本内容。 1. text和tail属性一般情况，可以用Element的text属性访问标签的文本。123456&gt;&gt;&gt; root = etree.Element('root')&gt;&gt;&gt; root.text = 'Hello, World!'&gt;&gt;&gt; print(root.text)Hello, World!&gt;&gt;&gt; print(etree.tostring(root))b'&lt;root&gt;Hello, World!&lt;/root&gt;' XML的标签一般是成对出现的，有开有关，但像HTML则可能出现单一的标签，如下面这段代码中的&lt;br/&gt;。 &lt;html&gt;&lt;body&gt;Text&lt;br/&gt;Tail&lt;/body&gt;&lt;/html&gt; Element类提供了tail属性支持单一标签的文本获取。123456789101112131415161718192021&gt;&gt;&gt; html = etree.Element('html')&gt;&gt;&gt; body = etree.SubElement(html, 'body')&gt;&gt;&gt; body.text = 'Text'&gt;&gt;&gt; print(etree.tostring(html))b'&lt;html&gt;&lt;body&gt;Text&lt;/body&gt;&lt;/html&gt;'&gt;&gt;&gt; br = etree.SubElement(body, 'br')&gt;&gt;&gt; print(etree.tostring(html))b'&lt;html&gt;&lt;body&gt;Text&lt;br/&gt;&lt;/body&gt;&lt;/html&gt;'# tail仅在该标签后面追加文本&gt;&gt;&gt; br.tail = 'Tail'&gt;&gt;&gt; print(etree.tostring(br))b'&lt;br/&gt;Tail'&gt;&gt;&gt; print(etree.tostring(html))b'&lt;html&gt;&lt;body&gt;Text&lt;br/&gt;Tail&lt;/body&gt;&lt;/html&gt;'# tostring方法增加method参数，过滤单一标签，输出全部文本&gt;&gt;&gt; print(etree.tostring(html, method='text'))b'TextTail' 2. XPath方式1234567# 方式一：过滤单一标签，返回文本&gt;&gt;&gt; print(html.xpath('string()'))TextTail# 方式二：返回列表，以单一标签为分隔&gt;&gt;&gt; print(html.xpath('//text()'))['Text', 'Tail'] 方法二获得的列表，每个元素都会带上它所属节点及文本类型信息，如下：12345678910111213141516171819&gt;&gt;&gt; texts = html.xpath('//text()'))&gt;&gt;&gt; print(texts[0])Text# 所属节点&gt;&gt;&gt; parent = texts[0].getparent() &gt;&gt;&gt; print(parent.tag)body&gt;&gt;&gt; print(texts[1], texts[1].getparent().tag)Tail br# 文本类型：是普通文本还是tail文本&gt;&gt;&gt; print(texts[0].is_text)True&gt;&gt;&gt; print(texts[1].is_text)False&gt;&gt;&gt; print(texts[1].is_tail)True 文件解析与输出 这部分讲述如何将XML文件解析为Element对象，以及如何将Element对象输出为XML文件。 1. 文件解析文件解析常用的有fromstring、XML和HTML三个方法。接受的参数都是字符串。12345678910111213141516171819202122&gt;&gt;&gt; xml_data = '&lt;root&gt;data&lt;/root&gt;'# fromstring方法&gt;&gt;&gt; root1 = etree.fromstring(xml_data)&gt;&gt;&gt; print(root1.tag)root&gt;&gt;&gt; print(etree.tostring(root1))b'&lt;root&gt;data&lt;/root&gt;'# XML方法，与fromstring方法基本一样&gt;&gt;&gt; root2 = etree.XML(xml_data)&gt;&gt;&gt; print(root2.tag)root&gt;&gt;&gt; print(etree.tostring(root2))b'&lt;root&gt;data&lt;/root&gt;'# HTML方法，如果没有&lt;html&gt;和&lt;body&gt;标签，会自动补上&gt;&gt;&gt; root3 = etree.HTML(xml_data)&gt;&gt;&gt; print(root3.tag)html&gt;&gt;&gt; print(etree.tostring(root3))b'&lt;html&gt;&lt;body&gt;&lt;root&gt;data&lt;/root&gt;&lt;/body&gt;&lt;/html&gt;' 2. 输出输出其实就是前面一直在用的tostring方法了，这里补充xml_declaration和encoding两个参数，前者是XML声明，后者是指定编码。123456789101112&gt;&gt;&gt; root = etree.XML('&lt;root&gt;&lt;a&gt;&lt;b/&gt;&lt;/a&gt;&lt;/root&gt;')&gt;&gt;&gt; print(etree.tostring(root))b'&lt;root&gt;&lt;a&gt;&lt;b/&gt;&lt;/a&gt;&lt;/root&gt;'# XML声明&gt;&gt;&gt; print(etree.tostring(root, xml_declaration=True))b"&lt;?xml version='1.0' encoding='ASCII'?&gt;\n&lt;root&gt;&lt;a&gt;&lt;b/&gt;&lt;/a&gt;&lt;/root&gt;"# 指定编码&gt;&gt;&gt; print(etree.tostring(root, encoding='iso-8859-1'))b"&lt;?xml version='1.0' encoding='iso-8859-1'?&gt;\n&lt;root&gt;&lt;a&gt;&lt;b/&gt;&lt;/a&gt;&lt;/root&gt;" ElementPath讲ElementPath前，需要引入ElementTree类，一个ElementTree对象可理解为一个完整的XML树，每个节点都是一个Element对象。而ElementPath则相当于XML中的XPath。用于搜索和定位Element元素。 这里介绍两个常用方法，可以满足大部分搜索、查询需求，它们的参数都是XPath语句：findall()：返回所有匹配的元素，返回列表find()：返回匹配到的第一个元素1234567891011121314151617&gt;&gt;&gt; root = etree.XML("&lt;root&gt;&lt;a x='123'&gt;aText&lt;b/&gt;&lt;c/&gt;&lt;b/&gt;&lt;/a&gt;&lt;/root&gt;")# 查找第一个b标签&gt;&gt;&gt; print(root.find('b'))None&gt;&gt;&gt; print(root.find('a').tag)a# 查找所有b标签，返回Element对象组成的列表&gt;&gt;&gt; [ b.tag for b in root.findall('.//b') ]['b', 'b']# 根据属性查询&gt;&gt;&gt; print(root.findall('.//a[@x]')[0].tag)a&gt;&gt;&gt; print(root.findall('.//a[@y]'))[] 原文地址：Python lxml教程-SKYue 参考资料 用lxml解析HTML Python中利用xpath解析HTML]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[urllib库的常见用法]]></title>
      <url>%2F2017%2F01%2F03%2Furllib%E5%BA%93%E7%9A%84%E5%B8%B8%E8%A7%81%E7%94%A8%E6%B3%95%2F</url>
      <content type="text"><![CDATA[先导入三个包：123import urllib.requestimport urllib.parseimport http.cookiejar 最简单的urlopen()123url = "http://www.baidu.com" response = urllib.request.urlopen(url)print(response.read()) 1. 构造headers12345user_agent = ''cookie = ''referer = ''host = ''content_type = '' 2. 构造Request实例对象12345url = 'http://www.baidu.com'values = &#123;'name' : 'Michael Foord', 'location' :'Northampton', 'language' :'Python' &#125;headers=&#123;'User-Agent':user_agent,'Cookie':cookie,'Referer':referer,'Host':host,'Content-Type':content_type&#125; 3. HTTP高级方法①使用Proxy代理12proxy_handler = urllib.request.ProxyHandler(&#123;'http': 'http://www.example.com:3128/'&#125;)opener = urllib.request.build_opener(proxy_handler) ②使用cookiejar1234cookie_jar = http.cookiejar.CookieJar()cookie_jar_handler = urllib.request.HTTPCookieProcessor(cookiejar=cookie_jar)opener.add_handler(cookie_jar_handler) ③发送123456# 1.安装全局opener，然后利用urlopen打开url urllib.request.install_opener(opener) response = urllib.request.urlopen(url)# 2.直接利用opener实例打开url:response = opener.open(url) 4. 抓取网页123456789data = urllib.parse.urlencode(values).encode('utf-8') req = urllib.request.Request(url,data,headers)#或req.add_header('Referer', 'http://www.baidu.com')# req.add_header('Origin', 'https://passport.weibo.cn')# req.add_header('User-Agent', 'Mozilla/6.0 (iPhone; CPU iPhone OS 8_0 like Mac OS X)...')response = urllib.request.urlopen(req,timeout=10)the_page = response.read().decode('utf-8') 或者这样：123data = urllib.parse.urlencode(&#123;"act": "login", "email": "xianhu@qq.com", "password": "123456"&#125;)request1 = urllib.request.Request(url,data=data,headers) # POST方法request2 = urllib.request.Request(url+"?%s" % data) # GET方法 其他方法1234#抓取网页中的图片：同样适用于抓取网络上的文件。右击鼠标，找到图片属性中的地址，然后进行保存。response = urllib.request.urlopen("http://ww3.sinaimg.cn/large/7d742c99tw1ee.jpg",timeout=120)with open("test.jpg", "wb") as file_img: file_img.write(response.read()) 123456# HTTP认证：即HTTP身份验证password_mgr = urllib.request.HTTPPasswordMgrWithDefaultRealm() # 创建一个PasswordMgrpassword_mgr.add_password(realm=None, uri=url, user='username', passwd='password') # 添加用户名和密码handler = urllib.request.HTTPBasicAuthHandler(password_mgr) # 创建HTTPBasicAuthHandleropener = urllib.request.build_opener(handler) # 创建opnerresponse = opener.open(url, timeout=10) # 获取数据 gzip压缩：在header中加入：&#39;request.add_header(&#39;Accept-encoding&#39;, &#39;gzip&#39;)&#39;这是关键:创建Request对象，添加一个 Accept-encoding 头信息告诉服务器你能接受 gzip 压缩数据 然后就是解压缩数据：1234567import StringIOimport gzipcompresseddata = f.read()compressedstream = StringIO.StringIO(compresseddata)gzipper = gzip.GzipFile(fileobj=compressedstream)print gzipper.read() 多线程爬取：1234567891011121314151617181920212223242526272829from threading import Threadfrom Queue import Queuefrom time import sleep# q是任务队列#NUM是并发线程总数#JOBS是有多少任务q = Queue()NUM = 2JOBS = 10#具体的处理函数，负责处理单个任务def do_somthing_using(arguments): print arguments#这个是工作进程，负责不断从队列取数据并处理def working(): while True: arguments = q.get() do_somthing_using(arguments) sleep(1) q.task_done()#fork NUM个线程等待队列for i in range(NUM): t = Thread(target=working) t.setDaemon(True) t.start()#把JOBS排入队列for i in range(JOBS): q.put(i)#等待所有JOBS完成q.join()]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[使用pytesseract识别简单验证码]]></title>
      <url>%2F2017%2F01%2F03%2F%E4%BD%BF%E7%94%A8pytesseract%E8%AF%86%E5%88%AB%E7%AE%80%E5%8D%95%E9%AA%8C%E8%AF%81%E7%A0%81%2F</url>
      <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233from PIL import Imageimport pytesseractfrom pytesseract import *rep=&#123;'O':'0', #替换列表 'I':'1','L':'1', 'Z':'2', 'S':'8' &#125;;def initTable(threshold=140): # 二值化函数 table = [] for i in range(256): if i &lt; threshold: table.append(0) else: table.append(1) return table#--------------------------------------------------------------------------------------im = Image.open('C:/Users/asus-pc/Desktop/Captcha.jpg') #1.打开图片im = im.convert('L') #2.将彩色图像转化为灰度图binaryImage = im.point(initTable(), '1') #3.降噪，图片二值化# binaryImage.show()text = image_to_string(binaryImage, config='-psm 7')#4.对于识别结果，常进行一些替换操作for r in rep: text = text.replace(r,rep[r])#5.打印识别结果print(text) 别人写的123456789101112131415161718192021222324252627282930313233from PIL import Imageimport pytesseractfrom pytesseract import *rep=&#123;'O':'0', #替换列表 'I':'1','L':'1', 'Z':'2', 'S':'8' &#125;;def initTable(threshold=140): # 二值化函数 table = [] for i in range(256): if i &lt; threshold: table.append(0) else: table.append(1) return table#--------------------------------------------------------------------------------------im = Image.open('C:/Users/asus-pc/Desktop/Captcha.jpg') #1.打开图片im = im.convert('L') #2.将彩色图像转化为灰度图binaryImage = im.point(initTable(), '1') #3.降噪，图片二值化# binaryImage.show()text = image_to_string(binaryImage, config='-psm 7')#4.对于识别结果，常进行一些替换操作for r in rep: text = text.replace(r,rep[r])#5.打印识别结果print(text)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[创建Python虚拟环境]]></title>
      <url>%2F2017%2F01%2F03%2F%E5%88%9B%E5%BB%BAPython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%2F</url>
      <content type="text"><![CDATA[1. 创建python虚拟环境： 最简单地创建python虚拟环境： 1virtualenv [虚拟环境名称] 如果你的系统里安装有不同版本的python，可以使用–python参数指定虚拟环境的python版本 1&gt;&gt; virtualenv [虚拟环境名称] --no-site-packages --python=C:/Users/asus-pc/AppData/Local/Programs/Python/Python35/python.exe 依赖系统环境的第三方软件包： 1&gt;&gt; virtualenv --system-site-packages [虚拟环境名称] 2. 进入虚拟环境目录，启动虚拟环境：123&gt;&gt; cd env1/&gt;&gt; source bin/activate 或 ./Scripts/activate #前面为Linux 后面为Windows&gt;&gt; python -V 3. 退出虚拟环境123&gt;&gt; deactivate或&gt;&gt; ./Scripts/deactivate 使用virtualenvwrapper 安装virtualenvwrapper 创建一个文件夹，用于存放所有的虚拟环境：mkdir ~/workspaces 设置环境变量，把下面两行添加到~/.bashrc里,然后就可以使用virtualenvwrapper了： 12export WORKON_HOME=~/workspacessource /usr/bin/virtualenvwrapper.sh 创建虚拟环境：mkvirtualenv [虚拟环境名称] 列出虚拟环境：lsvirtualenv -b 切换虚拟环境：workon [虚拟环境名称] 查看环境里安装了哪些包：lssitepackages 进入当前环境的目录：cdvirtualenv [子目录名] 进入当前环境的site-packages目录：cdsitepackages [子目录名] 控制环境是否使用global site-packages：toggleglobalsitepackages 复制虚拟环境：cpvirtualenv [source] [dest] 退出虚拟环境：deactivate 删除虚拟环境：rmvirtualenv [虚拟环境名称]]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[正则表达式]]></title>
      <url>%2F2017%2F01%2F03%2F%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%2F</url>
      <content type="text"><![CDATA[python re模块重要函数变量: 1. compile() 根据正则表达式字符串，创建模式的对象。 2. search() 在字符串中寻找模式。 3. match() 在字符串开始处匹配模式。 4. split() 根据模式的匹配项来分割字符串。 5. findall() 显示出字符串中模式的所有匹配项。 6. sub(old,new) 方法的功能是，用将所有old的匹配项用new替换掉。 7. escape() 将字符串中所有特殊正则表达式字符转义。 re.compile:可以把正则表达式编译成一个正则表达式对象。可以把那些经常使用的正则表达式编译成正则表达式对象，这样可以提高一定的效率。 12345import retext = "JGood is a handsome boy, he is cool, clever, and so on..."regex = re.compile(r'\w*oo\w*')print regex.findall(text) #查找所有包含'oo'的单词print regex.sub(lambda m: '[' + m.group(0) + ']', text) #将字符串中含有'oo'的单词用[]括起来。 re.split():会根据模式的匹配项来分割字符串，类似于我们字符串的split方法，不过它是用完整的正则表达式来替代了固定的分隔符。re.split(r&#39;\s+&#39;, text)将字符串按空格分割成一个单词列表。 re.findall:可以获取字符串中所有匹配的字符串。re.findall(r&#39;\w*oo\w*&#39;, text)获取字符串中，包含’oo’的所有单词。 re.sub()用于替换字符串中的匹配项。下面一个例子将字符串中的空格 ‘ ‘ 替换成 ‘-‘ : 123import retext = "JGood is a handsome boy, he is cool, clever, and so on..."print re.sub(r'\s+', '-', text) re.sub的函数原型为：re.sub(pattern, repl, string, count)其中第二个函数是替换后的字符串；本例中为’-‘第四个参数指替换个数。默认为0，表示每个匹配项都替换。re.sub还允许使用函数对匹配项的替换进行复杂的处理。如：re.sub(r&#39;\s&#39;, lambda m: &#39;[&#39; + m.group(0) + &#39;]&#39;, text, 0)将字符串中的空格’ ‘替换为’[ ]’。 re.match():尝试从字符串的开始匹配一个模式，如：下面的例子匹配第一个单词1234567891011121314import retext = "JGood is a handsome boy, he is cool, clever, and so on..."m = re.match(r"(\w+)\s", text)if m:print m.group(0), '\n', m.group(1)else:print 'not match'``` re.match的函数原型为：re.match(pattern, string, flags)第一个参数是正则表达式，这里为"(\w+)\s"，如果匹配成功，则返回一个Match，否则返回一个None；第二个参数表示要匹配的字符串；第三个参数是标致位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等。5. `re.search()`:会在字符串内查找模式匹配,只到找到第一个匹配然后返回，若字符串无匹配，则返回None。 `re.search()`的函数原型为:`re.search(pattern, string, flags)`.每个参数的含意与re.match一样。 import retext = “JGood is a handsome boy, he is cool, clever, and so on…”m = re.search(r’\shan(ds)ome\s’, text)if m:print m.group(0), m.group(1)else:print ‘not search’ ``` re.match与re.search的区别：re.match只匹配字符串的开始，如果字符串开始不符合正则表达式，则匹配失败，函数返回None；而re.search匹配整个字符串，直到找到一个匹配。 1. (\b)代表着单词的开头或结尾，也就是单词的分界处 2. (.)匹配除了换行符以外的任意字符 3. (*)代表的不是字符，不是位置，而是数量。它指定*前边的内容可连续重复使用任意次以使整个表达式得到匹配 4. 为了避免重复，可以这样写：0\d{2}-\d{8}。这里\d后面的{2}({8})的意思是前面\d必须连续重复匹配2次(8次) 5. (\s)匹配任意的空白符，包括空格，制表符(Tab)，换行符，中文全角空格等 6. (\w)匹配字母或数字或下划线或汉字等 表1.常用的元字符 代码 说明 . 匹配除换行符以外的任意字符 \w 匹配字母或数字或下划线或汉字 \s 匹配任意的空白符 \d 匹配数字 \b 匹配单词的开始或结束 ^ 匹配字符串的开始 $ 匹配字符串的结束 | 或指明一个非贪婪限定符 \ 将下一个字符标记为或特殊字符、或原义字符、或向后引用、或八进制转义符 () 标记一个子表达式的开始和结束位置。子表达式可以获取供以后使用。 [ 标记一个中括号表达式的开始 { 标记限定符表达式的开始 表2.常用的限定符 代码/语法 说明 * 重复零次或更多次 + 重复一次或更多次 ? 重复零次或一次,或指明一个非贪婪限定符 {n} 重复n次 {n,} 重复n次或更多次 {m,n} 重复m到n次 字符转义：使用\来取消这些字符的特殊意义。 字符类：要想查找数字，字母或数字，空白是很简单的，因为已经有了对应这些字符集合的元字符，但是如果你想匹配没有预定义元字符的字符集合(比如元音字母a,e,i,o,u),应该怎么办？很简单，你只需要在方括号里列出它们就行了，像[aeiou]就匹配任何一个英文元音字母，[.?!]匹配标点符号(.或?或!)。我们也可以轻松地指定一个字符范围，像[0-9]代表的含意与\d就是完全一致的：一位数字；同理[a-z0-9A-Z_]也完全等同于\w（如果只考虑英文的话）。 分枝条件:用|把不同的规则分隔开。 分组：如果想要重复多个字符又该怎么办？你可以用小括号来指定子表达式(也叫做分组)。 表3.常用的反义代码 代码/语法 说明 \W 匹配任意不是字母，数字，下划线，汉字的字符 \S 匹配任意不是空白符的字符 \D 匹配任意非数字的字符 \B 匹配不是单词开头或结束的位置 [^x] 匹配除了x以外的任意字符 [^aeiou] 匹配除了aeiou这几个字母以外的任意字符 后向引用：用于重复搜索前面某个分组匹配的文本。例如，\1代表分组1匹配的文本。\b(\w+)\b\s+\1\b可以用来匹配重复的单词，像go go, 或者kitty kitty。你也可以自己指定子表达式的组名。要指定一个子表达式的组名，请使用这样的语法：(?\w+)(或者把尖括号换成’也行：(?’Word’\w+)),这样就把\w+的组名指定为Word了。要反向引用这个分组捕获的内容，你可以使用\k,所以上一个例子也可以写成这样：\b(?\w+)\b\s+\k\b。 表4.常用分组语法 分类 代码/语法 说明 捕获 (exp) 匹配exp,并捕获文本到自动命名的组里 (?&lt;name&gt;exp) 匹配exp,并捕获文本到名称为name的组里，也可以写成(?&apos;name&apos;exp) (?:exp) 匹配exp,不捕获匹配的文本，也不给此分组分配组号 零宽断言 (?=exp) 匹配exp前面的位置 (?&lt;=exp) 匹配exp后面的位置 (?!exp) 匹配后面跟的不是exp的位置 (?&lt;!exp) 匹配前面不是exp的位置 注释 (?#comment) 这种类型的分组不对正则表达式的处理产生任何影响，用于提供注释让人阅读 零宽断言：用于查找在某些内容(但并不包括这些内容)之前或之后的东西。(?=exp)也叫零宽度正预测先行断言，它断言自身出现的位置的后面能匹配表达式exp。比如\b\w+(?=ing\b)，匹配以ing结尾的单词的前面部分(除了ing以外的部分)，如查找I’m singing while you’re dancing.时，它会匹配sing和danc。(?&lt;=exp)也叫零宽度正回顾后发断言，它断言自身出现的位置的前面能匹配表达式exp。比如(?&lt;=\bre)\w+\b会匹配以re开头的单词的后半部分(除了re以外的部分)，例如在查找reading a book时，它匹配ading。 负向零宽断言：只是想要确保某个字符没有出现，但并不想去匹配它时。零宽度负预测先行断言(?!exp)，断言此位置的后面不能匹配表达式exp。例如：\d{3}(?!\d)匹配三位数字，而且这三位数字的后面不能是数字；\b((?!abc)\w)+\b匹配不包含连续字符串abc的单词。同理，我们可以用(?&lt;!exp),零宽度负回顾后发断言来断言此位置的前面不能匹配表达式exp：(?&lt;![a-z])\d{7}匹配前面不是小写字母的七位数字。 注释:(?#comment) 贪婪与懒惰:贪婪匹配:匹配尽可能多的字符。懒惰匹配:匹配尽可能少的字符。用法：前面给出的限定符都可以被转化为懒惰匹配模式，只要在它后面加上一个问号?。a.*?b匹配最短的，以a开始，以b结束的字符串。如果把它应用于aabab的话，它会匹配aab（第一到第三个字符）和ab（第四到第五个字符）。为什么第一个匹配是aab（第一到第三个字符）而不是ab（第二到第三个字符）？简单地说，因为正则表达式有另一条规则，比懒惰／贪婪规则的优先级更高：最先开始的匹配拥有最高的优先权——The match that begins earliest wins。 表5.懒惰限定符 代码/语法 说明 *? 重复任意次，但尽可能少重复 +? 重复1次或更多次，但尽可能少重复 ?? 重复0次或1次，但尽可能少重复 {n,m}? 重复n到m次，但尽可能少重复 {n,}? 重复n次以上，但尽可能少重复 表6.常用的处理选项 名称 说明 IgnoreCase(忽略大小写) 匹配时不区分大小写。 Multiline(多行模式) 更改^和$的含义，使它们分别在任意一行的行首和行尾匹配，而不仅仅在整个字符串的开头和结尾匹配。(在此模式下,$的精确含意是:匹配\n之前的位置以及字符串结束前的位置.) Singleline(单行模式) 更改.的含义，使它与每一个字符匹配（包括换行符\n）。 IgnorePatternWhitespace(忽略空白) 忽略表达式中的非转义空白并启用由#标记的注释。 ExplicitCapture(显式捕获) 仅捕获已被显式命名的组。 表7.尚未详细讨论的语法 代码/语法 说明 \a 报警字符(打印它的效果是电脑嘀一声) \b 通常是单词分界位置，但如果在字符类里使用代表退格 \t 制表符，Tab \r 回车 \v 竖向制表符 \f 换页符 \n 换行符 \e Escape \0nn ASCII代码中八进制代码为nn的字符 \xnn ASCII代码中十六进制代码为nn的字符 \unnnn Unicode代码中十六进制代码为nnnn的字符 \cN ASCII控制字符。比如\cC代表Ctrl+C \A 字符串开头(类似^，但不受处理多行选项的影响) \Z 字符串结尾或行尾(不受处理多行选项的影响) \z 字符串结尾(类似$，但不受处理多行选项的影响) \G 当前搜索的开头 \p{name} Unicode中命名为name的字符类，例如\p{IsGreek} (?&gt;exp) 贪婪子表达式 (?&lt;x&gt;-&lt;y&gt;exp) 平衡组 (?im-nsx:exp) 在子表达式exp中改变处理选项 (?im-nsx) 为表达式后面的部分改变处理选项 (?(exp)yes|no) 把exp当作零宽正向先行断言，如果在这个位置能匹配，使用yes作为此组的表达式；否则使用no (?(exp)yes) 同上，只是使用空表达式作为no (?(name)yes|no) 如果命名为name的组捕获到了内容，使用yes作为表达式；否则使用no (?(name)yes) 同上，只是使用空表达式作为no]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[使用codecs模块解决文件编码问题]]></title>
      <url>%2F2017%2F01%2F03%2F%E4%BD%BF%E7%94%A8codecs%E6%A8%A1%E5%9D%97%E8%A7%A3%E5%86%B3%E6%96%87%E4%BB%B6%E7%BC%96%E7%A0%81%E9%97%AE%E9%A2%98%2F</url>
      <content type="text"><![CDATA[错误用例1234567891011# -*- coding: utf-8 -*-#http://www.qiushibaike.com/8hr/page/1?s=4603425import urllib2from bs4 import BeautifulSouppage=1xiubai=open(r'xiubai.txt','w+')for page in range(1,11): url="http://www.qiushibaike.com/8hr/page/"+str(page)+"?s=4603425" soup = BeautifulSoup(urllib2.urlopen(url).read()) for result in soup.findAll("div", "content", title=True): xiubai.write(result.text) 操作文件，读写数据，涉及到非ASCII的话，最好用codes模块操作，其会自动帮你处理不同的编码，效果最好。12345678import codecs;yourStrToSave = "your data string";# 'a+': read,write,append# 'w' : clear before, then writeoutputFp = codecs.open("outputFile.txt", 'w', 'utf-8');outputFp.write(yourStrToSave);outputFp.flush();outputFp.close(); 对于你此处获得html，并用bs处理html的过程，实际上更好的做法： 搞清楚本身html的charset&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=utf-8&quot; /&gt; 传递给BeautifulSoup去解析为soupsoup = BeautifuSoup(yourHtml,fromEncoding=&quot;GBK&quot;) 从find处理的soup节点，通过get_text()获得对应的内容 将获得的字符串内容，用codes保存到文件 完整代码123456789import codecs;# 'a+': read,write,append# 'w' : clear before, then writeoutputFp = codecs.open("outputFile.txt", mode='w', encoding='utf-8');for result in soup.findAll("div", "content", title=True): outputFp.write(result.get_text())outputFp.flush();outputFp.close(); 其中，bs中通过get_text()得到的字符串，已经是unicode了]]></content>
    </entry>

    
  
  
</search>
